---
title: "HW1 Answers"
author: "Yiwen Li"
date: "2024-02-13"
output:
  pdf_document: default
  html_document: default
---

# Problem 1

(a)

```{r, message=FALSE}
set.seed(123)

library(tidyverse)
library(neuralnet)
library(dplyr)
library(ggplot2)
library(caret)
library(class)
```

(b)

```{r}
# Load the data
my_data <- read.csv('income_evaluation.csv')
```

(c)

```{r}
# Randomly select 3000 row indices
sample_indices <- sample(nrow(my_data), 3000)
my_data <- my_data[sample_indices, ]
```

(d)

Numerical: Age, fnlwgt, education.num, capital.gain, capital.loss, hours.per.week, 
Categorical: Workclass, education, marital.status, occupation, relationship, race, sex, native.country, income

(e)

income; levels: $>50K$ and $<=50K$

(f)
```{r}
categorical_cols <- c("workclass", "education", "marital.status", "occupation", 
                      "relationship", "race", "sex", "native.country")  

# Step 2: Convert categorical variables to factors
my_data <- mutate_at(my_data, categorical_cols, as.factor)

# Step 3: Convert factors to appropriate numeric representations
# Using one-hot encoding (dummy variables)
my_data <- my_data %>%
  mutate_at(categorical_cols, ~as.integer(as.factor(.)))
```

(g)

```{r}
# Scale the variables
# variables_to_scale <- c("age", "fnlwgt", "education.num", "capital.gain", 
#                         "capital.loss", "hours.per.week")
variables_to_scale <- colnames(my_data)[1:14]
my_data[variables_to_scale] <- scale(my_data[variables_to_scale])
my_data$income <- ifelse(my_data$income == " >50K", 1, 0)
```

(h)

```{r}
# Split the data into training (70%) and testing sets (30%)
data_rows <- floor(0.7 * nrow(my_data))
train_indices <- sample(c(1:nrow(my_data)), data_rows)
train_data <- my_data[train_indices,]
test_data <- my_data[-train_indices,]
```

# Problem 2

(a)

```{r}
# Build the neural network with 1 layer and 5 neurons
model_1 <- neuralnet(
    income~., 
    data=train_data,
    hidden=c(5), 
    linear.output=FALSE,
    learningrate = 0.0001
)
```

(b)

```{r}
nn.results <- compute(model_1, test_data)
results <- data.frame(actual = test_data$income, prediction = nn.results$net.result)
results$prediction <- ifelse(results$prediction > 0.5, 1, 0)
```

(c)

```{r}
# Check if the predicted classes match the actual classes
check_1 <- results$prediction == results$actual

# Calculate accuracy as a percentage
accuracy_1 <- (sum(check_1) / nrow(test_data)) * 100
print(paste("Accuracy:", accuracy_1, "%"))

# Confusion Matrix
confusionMatrix <- confusionMatrix(as.factor(results$prediction), as.factor(results$actual))
print(confusionMatrix)
```

# Problem 3

(a)
```{r}
knn_model <- knn(train = train_data, test = test_data, cl = train_data$income, k = 10)
```

(b)
```{r}
misClassError <- mean(knn_model != test_data$income)
print(paste('Accuracy =', 1-misClassError))

# results_knn <- data.frame(actual = test_data$income, prediction = knn_model)
# check_2 <- results_knn$prediction == results_knn$actual
# accuracy_2 <- (sum(check_2) / nrow(test_data)) * 100
# print(paste("Accuracy:", accuracy_2, "%"))
```

```{r}
confusionMatrix <- confusionMatrix(as.factor(knn_model), as.factor(test_data$income))
print(confusionMatrix)
```

(c)
Based on the performance metrics, the KNN model is preferable for this classification task.  KNN outperforms the Neural Network model with an accuracy of 91.56% compared to 81.44%. This suggests that, overall, KNN makes more correct predictions for both positive and negative classes. KNN also has higher sensitivity, with 97.55% compared to Neural Network's 91.92%. In terms of specificity, KNN significantly outperformed the Neural Network, scoring 71.50% against 46.38%. 

# Problem 4

(a)
```{r}
km_model <- kmeans(my_data, centers = 2 , nstart = 30)
```

(b)
```{r}
my_data$cluster <- km_model$cluster
my_data$income_over_50K <- my_data$income == 1
proportions <- my_data %>%
  group_by(cluster) %>%
  summarise(ProportionOver50K = mean(income_over_50K))
print(proportions)
```

From the outputs, cluster 1 has a proportion of approximately 7.38% of individuals earning over \$50K, while cluster 2 has a significantly higher proportion, approximately 34.22%, of individuals earning over \$50K. The substantial difference in proportions between the two clusters indicates that the k-means clustering has effectively grouped individuals into two segments based on similarities in their data that correlate with income levels. Cluster 2 likely represents a group with higher overall income levels or features strongly associated with higher income.

(c)

Both Neural Networks (NNs) and K-Nearest Neighbors (KNN) are directly applicable to our classification task of predicting income levels because they are supervised learning algorithms that learn from labeled data. NNs offer the ability to capture deep, complex patterns and are scalable with large datasets, while KNN provides a simpler, intuitive approach that can be very effective with a well-chosen distance metric and k value. K-means clustering, while not a classification algorithm, can still be valuable in providing insights into the structure of the dataset. In terms of our goal of correctly classifying income level, KNN might be the preferred model to apply because of its outstanding performance on our data.



