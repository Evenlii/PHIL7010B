
\begin{frame}
\frametitle{Today}
\textbf{Learning goals}

\begin{itemize}
\item Basics KNN Algorithm
\item Bias-Variance Tradeoff
\item Implement KNN using R
\item Acknowledging the Challenges and Limitations
\end{itemize}

\end{frame}

\setbeamertemplate{sidebar left}[sidebar theme]
\section{}

\begin{frame}{Machine Learning Recap}
\begin{itemize}
    \item For many problems, it’s difficult to program the correct behavior by hand.
    \item Machine learning (ML) approach: program an algorithm to automatically learn from data, or from experience
    \item Three categories of ML: Supervised, Unsupervised, Reinforcement
    \item So far, most of the ML algorithms we've learned are supervised (Logistic Classification, Decision Tree and Random Forests, SVMs, ...)
\end{itemize}

\begin{center}
\includegraphics[width=0.7\textwidth]{Images/lecture 3/ml1.jpeg}
\end{center}
    
\end{frame}

\begin{frame}{Recap: Classification v.s. Reinforcement Learning}
\begin{itemize}
    \item \textbf{Classificatio}n: Classification is used for tasks where the goal is to assign a category or label to a given input. The input data is typically divided into predefined classes, and the model's objective is to correctly assign the input to one of these classes.
    \begin{itemize}
        \item Classification is a static decision-making task.
        \item Classification is often a supervised learning task, where we trained a fixed model on the labeled data.
    \end{itemize}
    \item \textbf{Reinforcement Learning (RL)}: RL is typically used for tasks where an agent learns to interact with an environment and make a sequence of decisions to maximize cumulative rewards. It's suitable for problems where the optimal action is not known in advance, and the agent needs to explore and learn through trial and error.
    \begin{itemize}
        \item RL receive feedback in the form of rewards or penalties based on their actions. RL models use this feedback to update their decision-making policy and improve over time.
        \item Reinforcement learning often deals with Markov decision processes (MDPs), where the future state depends only on the current state and action. This introduces complexity as RL models need to consider the impact of their actions on future states. 
    \end{itemize}
\end{itemize}
    
\end{frame}

\setbeamertemplate{sidebar left}[sidebar theme]
\section{K-Nearest Neighbors}

\begin{frame}{K-Nearest Neighbors}
\begin{itemize}
    \item Recall: A supervised machine learning algorithm is one that relies on labeled input data to learn a function that produces an appropriate output when given new unlabeled data.
    \item The K-nearest neighbors algorithm (KNN) is a very simple yet powerful \textbf{supervised} and \textbf{non-linear} algorithm
    \item It assigns a label to a new sample based on the labels of its k closest neighbors in the training set.
    \item KNN can be used for both classification and regression problems.
\end{itemize}
    
\end{frame}

\begin{frame}{K-Nearest Neighbors}
\begin{itemize}
    \item The idea behind the KNN classification algorithm is very simple: given a new sample, assign it to the class that is most common among its k nearest neighbors.
    \item The assumption behind KNN's classification rule is that similar things are near to each other.
 
\end{itemize}
\begin{center}
\includegraphics[width=0.7\textwidth]{Images/lecture 3/knn1.jpeg}
\end{center}
\end{frame}

\begin{frame}{K-Nearest Neighbors}
\begin{itemize}
    \item KNN is a supervised learning technique, so we should have a labeled dataset. Let’s say we have two classes: Class A (blue points) and Class B (green points). 
    \item A new data point (red) is given to us and we want to predict whether the new point belongs to Class A or Class B.
    \item Let’s first try K = 3. In this case, we have to find the three closest data points (aka three nearest neighbors) to the new (red) data point. 
    \item As can be seen from the left side, two of three closest neighbors belong to Class B (green) and one belongs to Class A (blue). So, we should assign the new point to Class B.
\end{itemize}
\begin{center}
\includegraphics[width=0.5\textwidth]{Images/lecture 3/knn2.jpeg}
\end{center}
\end{frame}

\begin{frame}{K-Nearest Neighbors}
\begin{itemize}
    \item Now let’s set K = 5. In this case, three out of the closest five points belong to Class A, so the new point should be classified as Class A.
\end{itemize}
\begin{center}
\includegraphics[width=0.5\textwidth]{Images/lecture 3/knn2.jpeg}
\end{center}
\begin{itemize}
    \item The above is just a simple illustration, how is the KNN algorithm being computed on the computer?
\end{itemize}
\end{frame}

\begin{frame}{K-Nearest Neighbors}
\begin{itemize}
    \item The training phase of the algorithm consists of only storing the training samples and their labels, i.e., no model is built from the data.
    \item In the prediction phase, KNN compute the distances between the test (query) point x and all the stored samples.
    \item It finds the k samples with the smallest computed distances, and assigns the majority label of these samples to x.
    \begin{itemize}
        \item For classification: assign the majority class label (majority voting)
        \item For regression: assign the average response
    \end{itemize}
    \item Thus, the algorithm requires:
    \begin{itemize}
        \item \textbf{Distance function}: To compute the similarities between examples
        \item \textbf{Parameter K}: number of nearest neighbors to look for
    \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Distance function}
\begin{itemize}
    \item Several ways to compute distances
    \item The choice depends on the type of the features in the data
    \item Real-valued features: Euclidean distance is commonly used
\end{itemize}
\begin{center}
\includegraphics[width=0.8\textwidth]{Images/lecture 3/equation.jpeg}
\end{center}
\end{frame}

\begin{frame}{Choosing K}
\begin{itemize}
    \item Nearest neighbors sensitive to noise or mis-labeled data (“class noise”).
    \item Solution? Smooth by having larger k nearest neighbors vote
\end{itemize}
\begin{center}
\includegraphics[width=0.7\textwidth]{Images/lecture 3/b3.jpeg}
\end{center}
\end{frame}

\begin{frame}{Bias-Variance Tradeoff}
Tradeoffs in choosing k?
\begin{itemize}
    \item Small k
    \begin{itemize}
        \item Good at capturing fine-grained patterns
        \item May be sensitive to random idiosyncrasies, in the training data, we call this \textbf{overfitting}.
    \end{itemize}
    \item Large k
    \begin{itemize}
        \item Makes stable predictions by averaging over lots of examples
        \item May fail to capture important regularities, we call this \textbf{underfitting}.
    \end{itemize}
\end{itemize}
Increasing k reduces variance but increases bias, we call it a Bias-Variance Tradeoff problem.
\begin{center}
\includegraphics[width=0.8\textwidth]{Images/lecture 3/bias.jpeg}
\end{center}
\end{frame}

\begin{frame}{Generalization error for k-NN}
\begin{itemize}
    \item We would like our algorithm to generalize to data it hasn’t seen before.
    \item How can we measure the generalization error (error rate on new examples)?
    \item Maybe we can use the test set to pick k? However, once we use the test set to pick k, it is no longer an unbiased way of measuring of how well we will do on \textbf{unseen} data.
    \item k is an example of a \textbf{hyperparameter}, something we can’t fit as part of the learning algorithm itself.
\end{itemize}
\end{frame}

\begin{frame}{Validation sets}
\begin{itemize}
    \item We can tune hyperparameters using a validation set, which is a third, unseen set of input-label pairs.
    \item The test set is used only at the very end, to measure the generalization performance of the final configuration.
\end{itemize}
\begin{center}
\includegraphics[width=0.8\textwidth]{Images/lecture 3/val1.jpeg}
\end{center}
\end{frame}

\begin{frame}{The Curse of Dimensionaility}
\begin{itemize}
    \item Every machine learning algorithm needs a dense data set in order to accurately predict over the entire data space.
    \item The special challenge with k-nearest neighbors is that it requires a point to be close in every single dimension.
    \item Some algorithms can create regressions based on single dimensions, and only need points to be close together along that axis. 
    \item KNN needs all points to be close along every axis in the data space. And each new axis added, by adding a new dimension, makes it harder and harder for two specific points to be close to each other in every axis.
    \item Low-dimensional visualizations are misleading. In high dimensions, “most” points are far apart.
\end{itemize}
\end{frame}


\setbeamertemplate{sidebar left}[sidebar theme]
\section{R Example}

\begin{frame}{R Example}

\begin{itemize}
    \item Now, let's build a KNN algorithm in R using the package 'class'.
    \item Again, we will use the Iris dataset. Recall that it contains measurements of four features: sepal length, sepal width, petal length, and petal width.
\end{itemize}
\begin{center}
\includegraphics[width=0.8\textwidth]{Images/lecture 2/1.jpeg}
\end{center}
\end{frame}

\begin{frame}{R Example}
\begin{center}
\includegraphics[width=0.7\textwidth]{Images/lecture 3/r1.jpeg}
\end{center}
\begin{itemize}
    \item We allocate 70\% of the data for training and the rest for testing. For the sake of simplicity, we will not use a validation set here.
    \item For KNN, we also applied the 'scale' function for feature scaling. 
    \item In KNN, the distance between data points is a crucial factor in predicting the class of a test point. If one feature has a much larger range of values than others, it can dominate the distance calculation, leading to biased results. Feature scaling ensures that each feature contributes approximately proportionately to the final distance.
\end{itemize}
\end{frame}

\begin{frame}{R Example}
\begin{center}
\includegraphics[width=0.7\textwidth]{Images/lecture 3/r2.jpeg}
\end{center}
\begin{itemize}
    \item We use the function 'knn()' to fit the KNN model used to fit the KNN model
    \item 'cl = train\_cl\$Species' specifies the class labels for the training data. 
    \item k = 3 sets the number of neighbors to consider in the KNN algorithm.
    \item After fitting the model, 'classifier\_knn' holds the predicted species for each instance in the test set. 
    \item 'misClassError' calculates the mean misclassification error by comparing these predictions to the actual species in 'test\_cl'. 
    \item Finally, '1 - misClassError' gives the accuracy of the model, which is printed.
\end{itemize}
\end{frame}

\begin{frame}{R Example}
Let's play around with the hyperparameter k and see how the model's accuracy changes.
\begin{center}
\includegraphics[width=0.5\textwidth]{Images/lecture 3/r4.png}
\end{center}

The highest accuracy is achieved when k is around 7. As k increases, there's a sharp decline in accuracy, indicating that including more neighbors is detrimental to the model's performance. This could be because adding more neighbors includes points that are further away from the query point, potentially introducing noise into the prediction.
\end{frame}

\begin{frame}{KNN - Wrap-up}
\begin{itemize}
    \item Advantages
    \begin{itemize}
        \item Easy to program
        \item No optimization or training required
        \item Classification accuracy can be very good; can outperform more complex models
    \end{itemize}
    \item Disadvantages
    \begin{itemize}
        \item Store all the training data in memory even at test time
        \item Expensive at test time
        \item May perform badly in high dimensions (curse of dimensionality)
    \end{itemize}
\end{itemize}
    
\end{frame}

\begin{frame}
\frametitle{Today}
\textbf{Learning goals}

\begin{itemize}
\item Basics KNN Algorithm
\item Bias-Variance Tradeoff
\item Implement KNN using R
\item Acknowledging the Challenges and Limitations
\end{itemize}

\end{frame}

\end{document}
