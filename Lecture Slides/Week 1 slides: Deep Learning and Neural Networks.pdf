\documentclass[xcolor=dvipsnames, 9pt]{beamer} % dvipsnames gives more built-in colors

% theme and color settings
\usetheme{Madrid}
\useoutertheme[hideothersubsections, left, height=0pt]{sidebar}
\useinnertheme{circles}

\definecolor{inseaddark}{RGB}{0,110,91} 
\definecolor{inseadlight}{RGB}{160,206,103}
\definecolor{fade}{HTML}{C1C7D0}
\definecolor{light_red}{HTML}{DCBCBC}
\definecolor{mid_red}{HTML}{B97C7C}
\definecolor{dark_red}{HTML}{8F2727}

\usecolortheme[named=inseaddark]{structure}
\usefonttheme[onlymath]{serif}

% some packages 
\usepackage{xcolor}
\usepackage{graphicx} 
\usepackage{multicol}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{tcolorbox}
\usepackage{colortbl}
\usepackage{amsmath}
\usepackage{tabularx}
\usepackage{subcaption}

%frontmatter
\title[Week 1]{\normalsize{PHIL 7010: Formal Methods for AI, Data, and Algorithms \vspace{0.5cm} \\Week 1 \\ Deep Learning and Neural Networks}}
\author[Boris Babic, HKU]{Boris Babic, \\HKU 100 Associate Professor of Data Science, Law and Philosophy}
\institute[]{}
\date{}

%\beamerdefaultoverlayspecification{<+->}

\begin{document}

\setbeamertemplate{sidebar left}{}

\begin{frame}
\titlepage
\begin{center}
\includegraphics[width=0.2\textwidth]{Images/HKU logo.jpeg}
\end{center}
\end{frame}

\begin{frame}
\frametitle{Today}
\textbf{Learning goals}

\begin{itemize}
\item Basics of Deep Learning and Neural Networks
\item Deep Learning Applications
\item Architecture of Neural Networks
\item Build and Interpret Neural Networks in R
\end{itemize}

\end{frame}

\setbeamertemplate{sidebar left}[sidebar theme]
\section{Deep Learning Overview}

\begin{frame}{Machine Learning v.s. Deep Learning}
\begin{itemize}
    \item We are now familiar with terms like Artificial Intelligence (AI) and Machine Learning (ML).
    \item Deep learning is one of the subsets of machine learning that uses deep learning algorithms to implicitly come up with important conclusions based on input data.
    \item Deep learning doesn’t rely on human expertise as much as traditional machine learning. Instead of using task-specific algorithms, it learns from representative examples. 
    \item For example, if you want to build a model that recognizes cats by species, you need to prepare a database that includes a lot of different cat images.
    \item Applications: 
    \begin{itemize}
        \item Speech recognition
        \item Facial recognition
        \item Natural language processing
        \item Recommender systems
    \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Some Breakthroughs}
\begin{center}
\includegraphics[width=1\textwidth]{Images/lecture 2/dl1.jpeg}
\end{center}
\end{frame}

\begin{frame}{Some Breakthroughs}
\begin{center}
\includegraphics[width=0.9\textwidth]{Images/lecture 2/dl2.jpeg}
\end{center}
\end{frame}

\setbeamertemplate{sidebar left}[sidebar theme]
\section{Neural Networks}

\begin{frame}{Perceptron}
\begin{itemize}
    \item Neural networks, also known as artificial neural networks (ANNs) or simulated neural networks (SNNs), are at the heart of deep learning algorithms.
    \item ANNs are composed of neurons, where each neuron individually performs only a simple computation.
    \item Before getting into Deep Learning, it’s a good idea to begin with the fundamental component of an ANN: the individual neuron.
    \item A neuron returns some output information from several input data, and in the late 1950s, Frank Rosenblatt and other researchers developed a class of ANN called \textbf{Perceptron}.
    \item A perceptron is a \textbf{linear classifier}; that is, it is an algorithm that classifies input by separating two categories with a straight line.
\end{itemize}
    
\end{frame}

\begin{frame}{Single Layer Perceptron}
\begin{itemize}
    \item The Single-Layer Perceptron (SLP) sets the groundwork for the fundamentals of modern Deep Learning architectures. 
    \item  They perform classification tasks, and can deal only with \textbf{linearly separable data}.
    \item It consists of a \textbf{single layer} of artificial neurons (also called perceptrons). Each neuron receives inputs, computes a weighted sum, and applies a threshold activation function to produce an output. There are no hidden layers in a single-layer perceptron.
\end{itemize}

\begin{center}
\includegraphics[width=0.6\textwidth]{Images/lecture 2/p1.jpeg}
\end{center}
\end{frame}

\begin{frame}{How SLP works}
SLP consists of five components: Inputs, Weights, Weighted sum (input), Linear/binary activation function, Bias.
\begin{itemize}
    \item Inputs are multiplied by weights， enabling the perceptron to assign more importance to some inputs than others.
    \item The weighted values are totaled to create the weighted sum.
    \item Bias is added as another adjustment to ensure accurate output.
    \item Based on the input (weighted sum), the activation function delivers the perceptron's output based on a  pre-defined threshold, where if f(z) is greater than a defined threshold $\theta$ we predict 1 and -1 otherwise.
\end{itemize}

\begin{center}
\includegraphics[width=0.6\textwidth]{Images/lecture 2/p2.jpeg}
\end{center}

\end{frame}

\begin{frame}{From SLP to MLP}
\begin{itemize}
    \item The SLP, during its initial stages, was celebrated as a transformative advancement in computational models. Its ability to linearly classify made it instrumental in a variety of domains. 
    \item However, SLPs represent weak models because they can only learn linearly separable functions, and as we know, the world is generally non-linear.
    \item It was not until the 1980s that some of these limitations were overcome with an improved concept called \textbf{Multi-Layer Perceptron (MLP)}.
    \item They also output diverse results on different runs, since all they care about is reaching some linear discrimination, not necessarily the most optimal one.
    \item MLPs were found as a solution to represent non-linearly separable functions, where the outputs of one layer are the inputs of the next one.
    \item The multifaceted MLPs, with their modest \textbf{hidden layers}, paved the way for the emergence of Deep Neural Networks (DNNs).


\end{itemize}
    
\end{frame}

\begin{frame}{The Rise of Deep Learning}
\begin{itemize}
    \item The "Neural Network Renaissance" (2000s)
    \begin{itemize}
        \item As the use of MLPs in machine learning tasks started to gain popularity, there was a resurgence of interest in neural networks, driven by advancements in computing power, larger datasets, and more efficient training algorithms.
        \item Researchers developed deep neural networks, by stacking multiple hidden layers. These networks showed improved performance in various applications.
    \end{itemize}
    \item Deep Learning (Mid-2010s - Present)
    \begin{itemize}
        \item Deep learning achieved remarkable success in various domains
        \item In the early 2010s, multiple studies recommended using a specific activation function (ReLu) in the neurons to train deeper neural networks efficiently.
        \item In 2016, AlphaGo — an AI developed by DeepMind — beat the Go world champion.
        \item In 2020, OpenAI released the third iteration of an AI specialized in producing texts: GPT3.
    \end{itemize}
\end{itemize}
    
\end{frame}

\begin{frame}{Deep Neural Networks}
\begin{itemize}
    \item Deep learning methods combine \textbf{multiple layers of neural networks to learn complex models}. Each neural network in a deep learning system is connected to other neural networks and to data to provide the computational resources needed to process the input.
    \item An DNN is typically comprised of an input layer, one or more hidden layers, and an output layer.
\end{itemize}
\begin{center}
\includegraphics[width=0.6\textwidth]{Images/lecture 2/ann1.jpeg}
\end{center}
\end{frame}

\begin{frame}{Components of Neural Networks - Neurons}
\begin{center}
\includegraphics[width=0.6\textwidth]{Images/lecture 2/ann1.jpeg}
\end{center}

\begin{itemize}
    \item \textbf{Neurons:} A neuron or a node of a neural network is a computing unit that receives information, performs simple calculations with it, and passes it further.
    \item In a large neural network with many neurons and connections between them, neurons are organized in layers. 
    \item All neurons in a net are divided into three groups:
    \begin{itemize}
        \item Input neurons that receive information from the outside world (Dark blue circles)
        \item Hidden neurons that process that information (Light blue circles in the middle)
        \item Output neurons that produce a conclusion (circles on the very right)
    \end{itemize}
\end{itemize}
    
\end{frame}

\begin{frame}{Components of Neural Networks - Layers}
\begin{itemize}
    \item But how do these neurons and layers process information into a decision we need?
    \item The mechanism behind was quite similar to logistic regression. 
    \item Recall that logistic regression is a binary model where the output y is either zero or one. An important step in a logistic model is that it applies a sigmoid function that will convert the continuous value y of the function into a boolean value (0 or 1) based on a specific threshold.
    \item Imagine neural networks as an expansive logistic model. Each neuron in the input layer corresponds to a predictor in our 'model,' and each connection between neurons in one layer and neurons in the next layer carries a \textbf{weight}, denoted as 'w.' These weights are akin to beta coefficients in logistic regression. 
    \item Instead of an intercept, the neural network includes \textbf{bias} 'b' allowing for more variations of weights to be stored. Biases add a richer representation of the input space to the model’s weights.
\end{itemize}
    
\end{frame}

\begin{frame}{Activation function}
\begin{itemize}
    \item In neural networks, each node in the hidden layers has its own set of weights 'w' and bias 'b' and learns unique insights into the relationship between feature variables and target variables.
    \item To perform transformations, every neuron has an \textbf{activation function} that outputs a boolean result.
    \item One of the commonly used activation functions is the sigmoid function. Other activation functions commonly used in DL include ReLU (Rectified Linear Unit) and Tanh (Hyperbolic Tangent).
\end{itemize}
\begin{center}
\includegraphics[width=0.6\textwidth]{Images/lecture 2/dl3.jpeg}
\end{center}
\end{frame}

\setbeamertemplate{sidebar left}[sidebar theme]
\section{Neural Networks - Simple Example}

\begin{frame}{Example}
\begin{itemize}
    \item Assume we are a credit card company targeting people without an adequate credit history. Using the historical data of its users, we want to decide whether a new applicant should be given a credit card or not. 
    \item Note: The latter 4 are “qualitative” variables, but for this illustration, let’s assume we can somehow quantify them. Also assume the neuron indicating '1' is a bias unit.
\end{itemize}
\begin{center}
\includegraphics[width=0.6\textwidth]{Images/lecture 2/ex1.jpeg}
\end{center}
    
\end{frame}

\begin{frame}{Example}
\begin{itemize}
    \item For creating a Neural Network, the first step is just stacking several units or neurons together to create a layer.
    \item Each blue circle is a processing unit (neuron), which performs the sum-product of the inputs (age, salary, education etc) and the weights, and applies an activation function (say ReLU) to give an output. Each neuron has a different set of weights and returns a different output.
\end{itemize}
\begin{center}
\includegraphics[width=0.6\textwidth]{Images/lecture 2/ex2.jpeg}
\end{center}
    
\end{frame}

\begin{frame}{Example}
\begin{itemize}
    \item Now, just like for this layer, our inputs were the data’s variables (age, salary, education etc), we can have this layer’s outputs (O1, O2, O3, O4) flowing into the next layer as its inputs.
    \item We can keep adding layers like this. To complete the Neural Network, we add an output neuron which takes in the outputs from the last layer as its input and returns the final output (the Yes/No decision for our use-case).
\end{itemize}
\begin{center}
\includegraphics[width=0.6\textwidth]{Images/lecture 2/ex3.jpeg}
\end{center}
    
\end{frame}

\begin{frame}{Example}
\begin{center}
\includegraphics[width=0.8\textwidth]{Images/lecture 2/ex4.jpeg}
\end{center}
\begin{itemize}
    \item Each blue or green circle above is a neuron having its weights. These weights have to be “learned” by the model, just like in Logistic Regression, where the model learned what slopes gave the results closest to the training data. 
    \item In DL, the networks learn the weights through a process called Back-propagation and Gradient Descent.
    \item Once the networks learn the optimum weights for all neurons, the input data is sent into the Neural Network, each neuron multiplying its inputs and weights, applying the activation on the sum and sending it to the next layer until we have the final result.
\end{itemize}

    
\end{frame}

\begin{frame}{Example}
\begin{center}
\includegraphics[width=0.7\textwidth]{Images/lecture 2/ex4.jpeg}
\end{center}
\begin{itemize}
    \item The blue layers (layers between the input and the final output neuron) are called the hidden layers. These are what makes the Model “Deep”.
    \item The hidden layer neurons try to capture the latent patterns within the data that will help the model in predicting the creditworthiness of the individual.
    \item For example, 
    \begin{itemize}
        \item One of the neurons could be inferring whether the applicant is earning enough to cover the average living expense of his city
        \item One neuron could be focusing on the company and designation to get an idea of their job security 
        \item Another could be focusing on, say, Age, Education, and Designation to get an idea about their performance on the job. etc.
    \end{itemize} 
\end{itemize}

    
\end{frame}

\begin{frame}{Example}
\begin{itemize}
    \item A neuron focusing on a certain variable just means that it’ll have higher magnitude (positive or negative) weights for that variable. The neurons will automatically adjust their weights using gradient descent algorithms to give outputs closest to the data on which they’re trained.
    \item These inferences help the NN to make a better decision about the applicant’s eligibility. Insights like the candidate’s salary adequacy, job security, performance, etc. will be better indicators to judge his/her eligibility than directly using the input variables(Age, Salary, Education etc).
    \item However, the mentioned inferences are just examples of what the neurons may be figuring out. In reality, it would be challenging to inspect the neural network (checking the neurons’ weights). It's also possible that we find patterns that may not be aligned to our general domain understanding or be directly interpretable by humans but have been identified by the NN as relevant for making predictions about the given data. Hence Neural Networks are also referred to as Black Box Models.
\end{itemize}
\end{frame}

\setbeamertemplate{sidebar left}[sidebar theme]
\section{Neural Networks - R Example}

\begin{frame}{R Example}

\begin{itemize}
    \item To build a simple Neural Network (NN) in R, we could use the 'neuralnet' package. This package offers a user-friendly interface to develop, train, and evaluate neural networks.
    \item The dataset we will use is the famous Iris dataset. It contains measurements of four features: sepal length, sepal width, petal length, and petal width.
    \item These measurements come from three different species of iris flowers: setosa, versicolor, and virginica.
    \item Our goal is to build a network on this dataset to make predictions on the species of iris flowers based on their physical measurements.
\end{itemize}
\begin{center}
\includegraphics[width=0.8\textwidth]{Images/lecture 2/1.jpeg}
\end{center}
\end{frame}

\begin{frame}{R Example}
\begin{center}
\includegraphics[width=0.7\textwidth]{Images/lecture 2/2.jpeg}
\end{center}
\begin{itemize}
    \item To begin, the first step is to split the data into training and testing sets.
    \item In this example, We allocate 70\% of the data for training and the rest for testing.
\end{itemize}
\end{frame}

\begin{frame}{R Example}
\begin{center}
\includegraphics[width=0.9\textwidth]{Images/lecture 2/3.jpeg}
\end{center}
\begin{itemize}
    \item We create a neural network model to predict the species (Species) based on the four features using the function 'neuralnet()'
    \item 'hidden=c(4,2)' specifies two hidden layers, the first layer with four neurons and the second with two neurons. 
    \item 'linear.output=FALSE' is used for classification tasks (non-linear).
\end{itemize}
\end{frame}

\begin{frame}{R Example}
\begin{center}
\includegraphics[width=0.7\textwidth]{Images/lecture 2/4.jpeg}
\end{center}
\begin{itemize}
    \item To view our model architecture, we will use the `plot` function. It requires a model object and `rep` argument. 
    \item 'rep = "best"' argument shows the best-performing network if multiple are trained.
    \item Each neuron is represented by a circle, and the lines connecting these circles to the input layer and to each other represent the weights. The weights are the numerical values that the network has learned during training.
\end{itemize}
\end{frame}

\begin{frame}{R Example}
\begin{center}
\includegraphics[width=0.7\textwidth]{Images/lecture 2/4.jpeg}
\end{center}
\begin{itemize}
    \item The blue lines represent the connections that were most influential for the final prediction in the "best" repetition of the model training.
    \item The three nodes on the right represent the output layer of the network, with each corresponding to one species.
    \item The output layer shows how the neural network combines the inputs from the hidden layers to make a prediction.
\end{itemize}
\end{frame}

\begin{frame}{R Example}
\begin{center}
\includegraphics[width=0.7\textwidth]{Images/lecture 2/4.jpeg}
\end{center}
\begin{itemize}
    \item At the bottom, the error of the neural network is noted as 0.001471, which is a measure of how well the network's predictions match the actual data. A lower error rate indicates better performance.
    \item "Steps: 2669" indicates the number of iterations the training algorithm took to converge to a solution.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Today}
\textbf{Learning goals}

\begin{itemize}
\item Basics of Deep Learning and Neural Networks
\item Deep Learning Applications
\item Architecture of Neural Networks
\item Build and Interpret Neural Networks in R
\end{itemize}

\end{frame}

\end{document}

