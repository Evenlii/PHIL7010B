\documentclass[xcolor=dvipsnames, 9pt]{beamer} % dvipsnames gives more built-in colors

% theme and color settings
\usetheme{Madrid}
\useoutertheme[hideothersubsections, left, height=0pt]{sidebar}
\useinnertheme{circles}

\definecolor{inseaddark}{RGB}{0,110,91} 
\definecolor{inseadlight}{RGB}{160,206,103}
\definecolor{fade}{HTML}{C1C7D0}
\definecolor{light_red}{HTML}{DCBCBC}
\definecolor{mid_red}{HTML}{B97C7C}
\definecolor{dark_red}{HTML}{8F2727}

\usecolortheme[named=inseaddark]{structure}
\usefonttheme[onlymath]{serif}

% some packages 
\usepackage{xcolor}
\usepackage{graphicx} 
\usepackage{multicol}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{tcolorbox}
\usepackage{colortbl}
\usepackage{amsmath}
\usepackage{tabularx}
\usepackage{subcaption}

%frontmatter
\title[Week 3]{\normalsize{PHIL 7010: Formal Methods for AI, Data, and Algorithms \vspace{0.5cm} \\Week 3 \\ K - Means Clustering}}
\author[Boris Babic, HKU]{Boris Babic, \\HKU 100 Associate Professor of Data Science, Law and Philosophy}
\institute[]{}
\date{}

%\beamerdefaultoverlayspecification{<+->}

\begin{document}

\setbeamertemplate{sidebar left}{}

\begin{frame}
\titlepage
\begin{center}
\includegraphics[width=0.2\textwidth]{Images/HKU logo.jpeg}
\end{center}
\end{frame}

\begin{frame}
\frametitle{Today}
\textbf{Learning goals}

\begin{itemize}
\item Concept of unsupervised learning
\item K-means clustering algorithm
\item Challenges and considerations in K-means
\item K-means clustering in R
\end{itemize}

\end{frame}

\setbeamertemplate{sidebar left}[sidebar theme]
\section{Unsupervised Learning}

\begin{frame}{Unsupervised Learning}
\begin{itemize}
    \item What can we do \textbf{without labels}?
    \item How can we even define what learning means without labels?
    \item Unsupervised learning is the study of learning without labels.
    \item It is the task of grouping, explaining, and finding structure in data.
\end{itemize}
\end{frame}

\begin{frame}{Motivating Example}
\begin{itemize}
\item Some examples of situations where you’d use unsupervised learning:
\begin{itemize}
    \item You want to understand how a scientific field has changed over time. You want to take a large database of papers and model how the distribution of topics changes from year to year. But what are the topics?
    \item You’re a biologist studying animal behavior, so you want to infer a high-level description of their behavior from the video. You don’t know the set of behaviors ahead of time.
    \item You want to reduce your energy consumption, so you take a time series of your energy consumption over time, and try to break it down into separate components (refrigerator, washing machine, etc.).
\end{itemize}
\item Common themes:
\begin{itemize}
    \item you have some data, and you want to infer some structure underlying the data.
    \item you have some data, and you want to be able to make more data that looks similar.
\end{itemize}
\end{itemize}
\end{frame}

\setbeamertemplate{sidebar left}[sidebar theme]
\section{Clustering}

\begin{frame}{Clustering}
\begin{itemize}
    \item Today we will look at the simplest type of unsupervised learning: clustering.
    \item Clustering is the task of organizing data into groups or clusters.
    \item We will study the simplest method for doing this: the K-means algorithm.
\end{itemize}
\end{frame}

\begin{frame}{Clustering}
\begin{itemize}
    \item Sometimes the data form clusters, where samples within a cluster are similar to each other, and samples in different clusters are dissimilar:
\begin{center}
\includegraphics[width=0.7\textwidth]{Images/lecture 4/c1.jpeg}
\end{center}
    \item Such a distribution is multimodal, since it has multiple modes, or regions of high probability mass.
    \item Grouping data points into clusters, with no observed labels, is called clustering. It is an unsupervised learning technique.
    \item E.g. clustering machine learning papers based on topic (deep learning, Bayesian models, etc.)
    \begin{itemize}
        \item But topics are never observed (unsupervised).
    \end{itemize}

\end{itemize}
\end{frame}

\begin{frame}{Clustering}
\begin{itemize}
    \item In centroid-based clustering like the K-Means clustering, each cluster is represented by a central vector, called the cluster center or centroid, which is not necessarily a member of the dataset. 
    \item The cluster centroid is typically defined as the mean of the points that belong to that cluster.
    \item Our goal in centroid-based clustering is to divide the data points into k clusters in such a way that the points are as close as possible to the centroids of the clusters they belong to.
\end{itemize}
\end{frame}

\begin{frame}{Clustering problem}
\begin{itemize}
    \item Assume that we are given a set of n data points: {$x_{1}$, …, $x_{n}$}, where each $x_{i}$ is a m-dimensional vector
    \item Assume each data point belongs to one of K clusters
    \item Assume the data points from same cluster are similar, i.e. close in Euclidean distance.
    \item we would like to partition the data into k sets (clusters) {$C_{1}$, …, $C_{k}$}, represented by centroids {$c_{1}$, …, $c_{k}$} such that 
    \begin{itemize}
        \item \textbf{Intra-cluster distances} are minimized: data points within the same cluster are as close as possible to one another.
        \item \textbf{Inter-cluster distances} are maximized: data points in different clusters are as far as possible from one another.
        
    \end{itemize}
\end{itemize}
\begin{center}
\includegraphics[width=0.7\textwidth]{Images/lecture 4/c2.jpeg}
\end{center}
\end{frame}

\begin{frame}{Clustering problem}
\begin{itemize}
    \item Before jumping into how the K-means algorithms work, let's first get a sense of how we measure the distances
    \item Keep in mind that we want to minimize the sum of squared distances from each point to its nearest centroid
    \item The sum of squared distances is called WCSS (Within-Cluster Sum of Squares) or inertia, and is defined as follows:
    \begin{center}
\includegraphics[width=0.9\textwidth]{Images/lecture 4/c3.jpeg}
\end{center}
    \item WCSS quantifies the compactness (or cohesiveness) of the clusters. The smaller WCSS we have within clusters, the more homogeneous (similar) the data points are within the same cluster.
\end{itemize}
\end{frame}

\setbeamertemplate{sidebar left}[sidebar theme]
\section{K-Means Algorithm}

\begin{frame}{The K-Means Algorithm}
The algorithm starts by choosing k initial centroids $c_{1}$, …, $c_{k}$. This can be done randomly by selecting k data points from the dataset or by using various other initialization methods (described later).
\vspace{5pt}

Then, the algorithm alternates between the following two steps:
\begin{itemize}
    \item \textbf{Assignment Step}: Assign each data point to the cluster with the nearest centroid, i.e., the centroid with the least squared Euclidean distance
    \item \textbf{Update Step}: For each cluster, compute a new centroid by calculating the mean of the data points assigned to that cluster.
\end{itemize}
These two steps are repeated until the cluster assignments no longer change or a specified number of iterations has been reached.
\end{frame}

\begin{frame}{Example}
Let’s demonstrate the k-means algorithm on the following dataset, which consists of 10 two-dimensional points, and let's set k = 3.
\begin{center}
\includegraphics[width=0.9\textwidth]{Images/lecture 4/c5.jpeg}
\end{center}
\end{frame}

\begin{frame}{Example}
Assume that the initial centroids are: $c_{1}$ = (1, 9), $c_{2}$ = (2, 2), and $c_{3}$ = (4, 7). The figure below shows the data points with the initial centroids marked by X:
\begin{center}
\includegraphics[width=0.9\textwidth]{Images/lecture 4/c6.jpeg} 
\end{center}
\end{frame}

\begin{frame}{First Iteration - Assignment}
We first compute the squared Euclidean distance from each data point to the three centroids
\begin{center}
\includegraphics[width=0.9\textwidth]{Images/lecture 4/c7.jpeg} 
\end{center}
Each data point is now assigned to the cluster with the nearest centroid (shown in yellow background). Therefore, the initial clusters are: $C_{1}$ = \{$p_{3}$\}, $C_{2}$ = \{$p_{1}$, $p_{2}$, $p_{4}$, $p_{8}$\}, $C_{3}$ = \{$p_{5}$, $p_{6}$, $p_{7}$, $p_{9}$, $p_{10}$\}.
\end{frame}

\begin{frame}{First Iteration - Update}
Next, we update the centroids of the three clusters. The centroid of $C_{1}$ is simply $p_{3}$, since this cluster contains only one point. The centroids of $C_{2}$ and $C_{3}$ are computed as the mean of the points that belong to each one:
\begin{center}
\includegraphics[width=0.6\textwidth]{Images/lecture 4/c11.jpeg} 
\end{center}

The new centroids are as follows:
\begin{center}
\includegraphics[width=0.6\textwidth]{Images/lecture 4/c8.jpeg} 
\end{center}
\end{frame}

\begin{frame}{Second Iteration - Assignment}
Next, we compute the distances of the data points to the new centroids and reassign them to the clusters with the nearest centroid:
\begin{center}
\includegraphics[width=0.9\textwidth]{Images/lecture 4/c9.jpeg} 
\end{center}
The new clusters are: $C_{1}$ = \{$p_{3}$, $p_{5}$, $p_{6}$\}, $C_{2}$ = \{$p_{1}$, $p_{2}$, $p_{4}$\}, $C_{3}$ = \{$p_{7}$, $p_{8}$, $p_{9}$, $p_{10}$\}.
\end{frame}

\begin{frame}{Second Iteration - Update}
We now recalculate the centroids of these clusters:
\begin{center}
\includegraphics[width=0.6\textwidth]{Images/lecture 4/c12.jpeg} 
\end{center}

The new clusters after the second iteration:
\begin{center}
\includegraphics[width=0.6\textwidth]{Images/lecture 4/c10.jpeg} 
\end{center}
\end{frame}


\begin{frame}{Convergence}
We will keep repeating the processes until the cluster assignments no longer change. The algorithm is guaranteed to converge to a solution. However, \textbf{the solution might be a local minimum of the WCSS, and not necessarily the global optimum}.
\begin{itemize}
    \item \textbf{Global Minimum}: the absolute lowest possible value of the WCSS. This means that the data points are partitioned in such a way that they couldn't be arranged into any other clusters to achieve a lower WCSS.
    \item \textbf{Local Minimum}: a configuration where any small adjustment to the cluster assignments or centroids would increase the WCSS, but it's not necessarily the configuration with the lowest possible WCSS. In other words, while the solution is locally optimal (i.e., no nearby configuration is better), there may be other configurations (far from the current one) with a lower WCSS.
\end{itemize}
\begin{center}
\includegraphics[width=0.4\textwidth]{Images/lecture 4/c4.jpeg} 
\end{center}
\end{frame}

\begin{frame}{K-Means Local Minima}
\begin{itemize}
    \item The K-mans clustering algorithm is guaranteed to converge (i.e., reach a point where further iterations do not change the clusters) because each iteration reduces the WCSS or leaves it unchanged, and there's a finite limit to how much it can be reduced.
    \item However, the algorithm is very sensitive to our initial choice of the centroids, depending on which values we choose for our initial centroids we may obtain differing results.
    \item If the centroids are initialized poorly, the algorithm may get "stuck" in a local minimum where it can no longer reduce the WCSS by changing cluster assignments or centroids, even though a better (lower WCSS) global solution exists. 
    \item Solution? Run the algorithm using different initializations of centroids several times and to pick the results of the run that yielded the lowest value of WCSS
\end{itemize}
\end{frame}

\setbeamertemplate{sidebar left}[sidebar theme]
\section{R implementation}

\begin{frame}{Implement K-Means in R}
To perform k-means clustering in R we can use the built-in 'kmeans()' function:

\begin{center}
\includegraphics[width=0.6\textwidth]{Images/lecture 4/r.jpeg} 
\end{center}

where:
\begin{itemize}
    \item \textbf{df}: Name of the dataset.
    \item \textbf{centers}: The number of clusters.
    \item \textbf{nstart}: Run k-means with n different random starting assignments (e.g. 25 in this case) and, then, R will automatically choose the best results total within-cluster sum of squares.
\end{itemize}


    
\end{frame}

\setbeamertemplate{sidebar left}[sidebar theme]
\section{Pros and Cons}

\begin{frame}{K-means: Pros and Cons}
Pros:
\begin{itemize}
    \item Computationally efficient
    \item Works well even with large datasets
    \item Convergence guaranteed
    \item Easy to interpret
\end{itemize}
Cons:
\begin{itemize}
    \item Sensitivity to Outliers
    \item Dependence on Initialization
\end{itemize}
    
\end{frame}

\begin{frame}
\frametitle{Today}
\textbf{Learning goals}

\begin{itemize}
\item Concept of unsupervised learning
\item K-means clustering algorithm
\item Challenges and considerations in K-means
\item K-means clustering in R
\end{itemize}

\end{frame}

\end{document}


